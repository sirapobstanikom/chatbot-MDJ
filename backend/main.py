from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional
import uvicorn
import os
from dotenv import load_dotenv

# === OpenAI SDK v1 ===
from openai import OpenAI, APIError, RateLimitError, AuthenticationError

import anyio
import asyncio
import datetime

# -----------------------------------------------------------------------------
# Config
# -----------------------------------------------------------------------------
try:
    load_dotenv(override=True)
    print("‚úÖ Environment variables loaded successfully")
except Exception as e:
    print(f"‚ùå Error loading .env file: {e}")

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

if OPENAI_API_KEY:
    print(f"‚úÖ OpenAI API Key: {OPENAI_API_KEY[:20]}...")
    print(f"‚úÖ OpenAI Model: {OPENAI_MODEL}")
else:
    print("‚ùå No OpenAI API key found")
    print("‚ö†Ô∏è  Running in Basic Mode")

# -----------------------------------------------------------------------------
# App & CORS
# -----------------------------------------------------------------------------
app = FastAPI(title="Chatbot API", version="1.2.0 (memory+autosummary)")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*", "http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# -----------------------------------------------------------------------------
# Schemas
# -----------------------------------------------------------------------------
class ChatRequest(BaseModel):
    session_id: Optional[str] = None
    message: str

class ChatResponse(BaseModel):
    response: str
    timestamp: str
    session_id: str

class HistoryResponse(BaseModel):
    session_id: str
    messages: List[Dict[str, str]]

# -----------------------------------------------------------------------------
# System prompt
# -----------------------------------------------------------------------------
SYSTEM_MESSAGE = (
    "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI Chatbot ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ "
    "‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ñ‡∏≤‡∏° "
    "‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå "
    "‡πÉ‡∏ä‡πâ‡πÇ‡∏ó‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£‡πÅ‡∏•‡∏∞‡∏™‡∏∏‡∏†‡∏≤‡∏û"
)

# -----------------------------------------------------------------------------
# In-memory stores
# -----------------------------------------------------------------------------
CHAT_STORE: Dict[str, List[Dict[str, str]]] = {}
STORE_LOCK = asyncio.Lock()

# ‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß/‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏ó‡∏≠‡∏ô
MAX_TURNS = 12                  # ‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏≠‡∏ö‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (user+assistant ‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° system)
SOFT_CHAR_LIMIT = 12000         # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏£‡∏ß‡∏°‡πÄ‡∏Å‡∏¥‡∏ô ‡∏à‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô‡∏•‡∏á‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
KEEP_RECENT_TURNS_AFTER_SUM = 4 # ‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡πâ‡∏≤‡∏¢‡πÜ ‡πÑ‡∏ß‡πâ‡∏´‡∏•‡∏±‡∏á‡∏™‡∏£‡∏∏‡∏õ

# -----------------------------------------------------------------------------
# Helpers
# -----------------------------------------------------------------------------
def _init_session_if_needed(session_id: str):
    if session_id not in CHAT_STORE:
        CHAT_STORE[session_id] = [{"role": "system", "content": SYSTEM_MESSAGE}]

def _trim_turns(session_id: str):
    msgs = CHAT_STORE.get(session_id, [])
    if not msgs:
        return
    system = msgs[0:1]
    turns = msgs[1:]
    if len(turns) > MAX_TURNS:
        turns = turns[-MAX_TURNS:]
    CHAT_STORE[session_id] = system + turns

def _total_chars(session_id: str) -> int:
    return sum(len(m.get("content", "")) for m in CHAT_STORE.get(session_id, []))

def resolve_session_id(body_sid: Optional[str], req: Request) -> str:
    return (body_sid or "").strip() or (req.headers.get("X-Session-Id", "").strip()) or "default"

def _summary_prompt(history: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤ (‡∏¢‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô ‡∏ä‡∏±‡∏î ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ)
    """
    # ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ user/assistant (‡∏Ç‡πâ‡∏≤‡∏° system ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å)
    lines = []
    for m in history[1:]:
        role = m["role"]
        if role in ("user", "assistant"):
            lines.append(f"{role.upper()}: {m['content']}")
    joined = "\n".join(lines)[:8000]  # safety cut
    return [
        {"role": "system", "content": "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏¢‡πà‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ñ‡∏≤‡∏ß‡∏£ ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏ö‡∏ö‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÄ‡∏õ‡πá‡∏ô bullet/‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡πâ‡∏≠‡∏™‡∏£‡∏∏‡∏õ/‡∏Ç‡πâ‡∏≠‡πÄ‡∏ó‡πá‡∏à‡∏à‡∏£‡∏¥‡∏á/‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏≥‡∏ï‡πà‡∏≠‡πÑ‡∏õ"},
        {"role": "user", "content": f"‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 12 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î) ‡πÅ‡∏•‡∏∞‡πÄ‡∏ô‡πâ‡∏ô‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏¢‡∏ï‡πà‡∏≠:\n\n{joined}"}
    ]

def _insert_summary_as_system(session_id: str, summary_text: str):
    """
    ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡πÅ‡∏ä‡∏ó‡πÉ‡∏´‡πâ‡∏°‡∏µ system ‡πÄ‡∏î‡∏¥‡∏° + system ‡∏™‡∏£‡∏∏‡∏õ + ‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡πâ‡∏≤‡∏¢‡πÜ ‡∏≠‡∏µ‡∏Å‡∏ô‡∏¥‡∏î
    """
    msgs = CHAT_STORE.get(session_id, [])
    if not msgs:
        _init_session_if_needed(session_id)
        msgs = CHAT_STORE[session_id]

    system = msgs[0:1]
    turns = msgs[1:]
    keep_tail = turns[-(KEEP_RECENT_TURNS_AFTER_SUM*2):]  # ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì user/assistant ‡∏™‡∏•‡∏±‡∏ö‡∏Å‡∏±‡∏ô

    summarized = system + [
        {"role": "system", "content": f"[‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤]\n{summary_text.strip()}"}
    ] + keep_tail

    CHAT_STORE[session_id] = summarized
    _trim_turns(session_id)

def _compress_if_needed(session_id: str):
    """
    ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô SOFT_CHAR_LIMIT ‡∏à‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞‡∏¢‡πà‡∏≠‡∏Å‡πà‡∏≠‡∏ô (‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å)
    """
    if _total_chars(session_id) <= SOFT_CHAR_LIMIT:
        return

    history = CHAT_STORE.get(session_id, [])
    if not history or len(history) < 2:
        return

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡πâ‡∏™‡∏£‡∏∏‡∏õ
    try:
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=_summary_prompt(history),
            max_tokens=320,
            temperature=0.2,
        )
        summary = (resp.choices[0].message.content or "").strip()
        _insert_summary_as_system(session_id, summary)
    except Exception as e:
        # ‡∏ñ‡πâ‡∏≤‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏•‡πâ‡∏° ‚Äî ‡πÅ‡∏Ñ‡πà‡∏ö‡∏µ‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ó‡πâ‡∏≤‡∏¢‡πÜ
        msgs = CHAT_STORE.get(session_id, [])
        system = msgs[0:1]
        turns = msgs[1:]
        CHAT_STORE[session_id] = system + turns[-(KEEP_RECENT_TURNS_AFTER_SUM*2):]
        _trim_turns(session_id)

# -----------------------------------------------------------------------------
# OpenAI call with retry-on-context
# -----------------------------------------------------------------------------
def _call_openai(session_id: str) -> str:
    if not client:
        return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ OpenAI API key ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"

    try:
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=CHAT_STORE[session_id],
            max_tokens=500,
            temperature=0.7,
        )
        return (resp.choices[0].message.content or "").strip()
    except APIError as e:
        msg = str(e)
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ñ‡∏≠‡∏ô‡πÄ‡∏ó‡πá‡∏Å‡∏ã‡πå ‡πÉ‡∏´‡πâ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡πâ‡∏ß‡∏¢‡∏¥‡∏á‡πÉ‡∏´‡∏°‡πà‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á
        if "maximum context length" in msg.lower() or "too many tokens" in msg.lower():
            _compress_if_needed(session_id)
            resp2 = client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=CHAT_STORE[session_id],
                max_tokens=500,
                temperature=0.7,
            )
            return (resp2.choices[0].message.content or "").strip()
        raise
    except AuthenticationError:
        return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö OpenAI API key ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á"
    except RateLimitError:
        return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô OpenAI API ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á"
    except Exception as e:
        return f"‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î: {str(e)}"

async def get_ai_reply(session_id: str) -> str:
    return await anyio.to_thread.run_sync(_call_openai, session_id)

# -----------------------------------------------------------------------------
# Fallback
# -----------------------------------------------------------------------------
def fallback_reply(user_message: str) -> str:
    low = user_message.lower()
    if "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ" in low or "hello" in low:
        return "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö! ‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏™‡∏π‡πà AI Chatbot ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤ üòä"
    elif "‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£" in low or "what's your name" in low:
        return "‡∏â‡∏±‡∏ô‡∏ä‡∏∑‡πà‡∏≠ AI Chatbot ‡∏Ñ‡∏£‡∏±‡∏ö! ‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡∏Ñ‡∏∏‡∏ì"
    elif "‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠" in low or "help" in low:
        return "‡∏â‡∏±‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πá‡πÑ‡∏î‡πâ!"
    elif "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì" in low or "thank" in low:
        return "‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö! ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏µ‡∏Å‡πÑ‡∏´‡∏°‡∏Ñ‡∏£‡∏±‡∏ö?"
    elif "‡∏•‡∏≤‡∏Å‡πà‡∏≠‡∏ô" in low or "bye" in low:
        return "‡∏•‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö! ‡∏Ç‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö üëã"
    else:
        return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏â‡∏±‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ ‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°‡∏Ñ‡∏£‡∏±‡∏ö?"

# -----------------------------------------------------------------------------
# Routes
# -----------------------------------------------------------------------------
@app.get("/")
async def root():
    return {"message": "Chatbot API (memory+autosummary) is running!"}

@app.get("/health")
async def health_check():
    return {"status": "healthy"}

@app.get("/openai-status")
async def openai_status():
    if not client:
        return {"status": "not_configured", "message": "OpenAI API key ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤", "openai_available": False}
    try:
        _ = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[{"role": "user", "content": "ping"}],
            max_tokens=5,
        )
        return {"status": "connected", "message": "‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ OpenAI ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à", "openai_available": True, "model": OPENAI_MODEL}
    except Exception as e:
        return {"status": "error", "message": f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}", "openai_available": False}

@app.get("/history/{session_id}", response_model=HistoryResponse)
async def get_history(session_id: str):
    async with STORE_LOCK:
        _init_session_if_needed(session_id)
        return HistoryResponse(session_id=session_id, messages=CHAT_STORE[session_id])

@app.delete("/history/{session_id}")
async def reset_history(session_id: str):
    async with STORE_LOCK:
        CHAT_STORE[session_id] = [{"role": "system", "content": SYSTEM_MESSAGE}]
    return {"status": "reset", "session_id": session_id}

@app.post("/chat", response_model=ChatResponse)
async def chat(req_body: ChatRequest, req: Request):
    try:
        session_id = resolve_session_id(req_body.session_id, req)

        async with STORE_LOCK:
            _init_session_if_needed(session_id)
            CHAT_STORE[session_id].append({"role": "user", "content": req_body.message})
            _trim_turns(session_id)
            _compress_if_needed(session_id)

        try:
            reply = await get_ai_reply(session_id)
        except Exception:
            reply = fallback_reply(req_body.message)

        async with STORE_LOCK:
            CHAT_STORE[session_id].append({"role": "assistant", "content": reply})
            _trim_turns(session_id)

        ts = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        return ChatResponse(response=reply, timestamp=ts, session_id=session_id)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# -----------------------------------------------------------------------------
# Main
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    # ‡πÇ‡∏õ‡∏£‡∏î‡∏±‡∏Å‡∏ä‡∏±‡∏ô‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ process manager (‡πÄ‡∏ä‡πà‡∏ô uvicorn/gunicorn) ‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ
    uvicorn.run(app, host="0.0.0.0", port=8000)
